# proto-file: deeplab2/config.proto
# proto-message: ExperimentOptions
#
# Panoptic-DeepLab with ResNet-50 and output stride 32.
#
############### PLEASE READ THIS BEFORE USING THIS CONFIG ###############
# Before using this config, you need to update the following fields:
# - experiment_name: Use a unique experiment name for each experiment.
# - initial_checkpoint: Update the path to the initial checkpoint.
# - train_dataset_options.file_pattern: Update the path to the
#   training set. e.g., your_dataset/train*.tfrecord
# - eval_dataset_options.file_pattern: Update the path to the
#   validation set, e.g., your_dataset/eval*.tfrecord
# - (optional) set merge_semantic_and_instance_with_tf_op: true, if you
#   could successfully compile the provided efficient merging operation
#   under the folder `tensorflow_ops`.
#########################################################################
#
# This config provides an example to launch GPU training with
# `merge_semantic_and_instance_with_tf_op` = false, which will NOT invoke
# our efficient merging operation. For faster inference speed, please
# compile the provided `tensorflow_ops` and then set
# `merge_semantic_and_instance_with_tf_op` to true.
#
# References:
# For ResNet, see
# - Kaiming He, et al. "Deep Residual Learning for Image Recognition."
#   In CVPR, 2016.
# For Panoptic-DeepLab, see
# - Bowen Cheng, et al. "Panoptic-DeepLab: A Simple, Strong, and Fast Baseline
#   for Bottom-Up Panoptic Segmentation." In CVPR, 2020.

# Use a unique experiment_name for each experiment.
# experiment_name: "pk-expt-4-panoptic-deeplab-resnet50-with-32-quarter-crop-skip-resize-use-scaling-30k-steps-LABEL_DIVISOR_FIX"
experiment_name: "pk-debug-1-panoptic-deeplab"
model_options {
  # Update the path to the initial checkpoint (e.g., ImageNet
  # pretrained checkpoint).
  initial_checkpoint: "/home/pkadubandi/GH/PradeepKadubandi/waymo-challenge/pretrained_checkpoints/resnet50_imagenet1k_strong_training_strategy/ckpt-350.index"
  backbone {
    name: "resnet50"
    output_stride: 32
  }
  decoder {
    feature_key: "res5"
    decoder_channels: 256
    aspp_channels: 256
    atrous_rates: 3
    atrous_rates: 6
    atrous_rates: 9
  }
  panoptic_deeplab {
    low_level {
      feature_key: "res3"
      channels_project: 64
    }
    low_level {
      feature_key: "res2"
      channels_project: 32
    }
    instance {
      low_level_override {
        feature_key: "res3"
        channels_project: 32
      }
      low_level_override {
        feature_key: "res2"
        channels_project: 16
      }
      instance_decoder_override {
        feature_key: "res5"
        decoder_channels: 128
        atrous_rates: 3
        atrous_rates: 6
        atrous_rates: 9
      }
      center_head {
        output_channels: 1
        head_channels: 32
      }
      regression_head {
        output_channels: 2
        head_channels: 32
      }
    }
    semantic_head {
      output_channels: 29
      head_channels: 256
    }
  }
}
trainer_options {
  save_checkpoints_steps: 6000
  save_summaries_steps: 100
  steps_per_loop: 100
  loss_options {
    semantic_loss {
      name: "softmax_cross_entropy"
      weight: 1.0
      top_k_percent: 0.2
    }
    center_loss {
      name: "mse"
      weight: 200
    }
    regression_loss {
      name: "l1"
      weight: 0.01
    }
  }
  solver_options {
    base_learning_rate: 0.00025
    training_number_of_steps: 30000
  }
}
train_dataset_options {
  dataset: "wod_pvps_image_panoptic_seg"
  file_pattern: "/home/pkadubandi/data/waymo-open-dataset/v_2_0_0/training/vip_deeplab_format/*.tfrecord"
  crop_size: 320 # On my machine with 2 GPUS of 16 GB memory, at full resolution (1280 x 1920), I get OOM error for batch_size = 4 when using resnet50 backbone.
  crop_size: 480,
  batch_size: 16,
  augmentations {
    min_scale_factor: 0.5
    max_scale_factor: 2.0
    scale_factor_step_size: 0.1
  }
  min_resize_value: 0
  max_resize_value: 0
}
eval_dataset_options {
  dataset: "wod_pvps_image_panoptic_seg"
  # Update the path to validation set.
  file_pattern: "/home/pkadubandi/data/waymo-open-dataset/v_2_0_0/validation/vip_deeplab_format/*.tfrecord"
  batch_size: 1
  crop_size: 320
  crop_size: 480
  max_resize_value: 0
  max_resize_value: 0
  # TODO: Add options to make the evaluation loss comparable to the training loss.
  # increase_small_instance_weights: true
  # small_instance_weight: 3.0
}
evaluator_options {
  continuous_eval_timeout: -1
  eval_interval: 10000
  eval_steps: 1000
  stuff_area_limit: 4096
  center_score_threshold: 0.1
  nms_kernel: 41
  save_predictions: false
  save_raw_predictions: false
  # Use pure tf functions (i.e., no CUDA kernel) to merge semantic and
  # instance maps. For faster speed, compile TensorFlow with provided kernel
  # implementation under the folder `tensorflow_ops`, and set
  # merge_semantic_and_instance_with_tf_op to true.
  merge_semantic_and_instance_with_tf_op: false
}
